# -*- coding: utf-8 -*-
"""EDA-Walmart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UI0-aaaFJ2zxGOXsljtgYGxWkMLJzlf

# Descripcion de Datos

Se proporcionan datos históricos de ventas de 45 tiendas Walmart ubicadas en diferentes regiones. Cada tienda contiene una cantidad de departamentos, y usted tiene la tarea de predecir las ventas de todo el departamento para cada tienda.

Además, Walmart realiza varios eventos promocionales de rebajas a lo largo del año. Estas rebajas preceden a los días festivos destacados, los cuatro más importantes son el Super Bowl, el Día del Trabajo, el Día de Acción de Gracias y la Navidad. Las semanas que incluyen estos días festivos se ponderan cinco veces más en la evaluación que las semanas que no son días festivos. Parte del desafío que presenta esta competencia es modelar los efectos de las rebajas en estas semanas de vacaciones en ausencia de datos históricos completos/ideales.

El conjunto de datos es el siguiente:

**-tiendas.csv**

Este archivo contiene información anonimizada sobre las 45 tiendas, indicando el tipo y tamaño de la tienda.

**-train.csv**
Estos son los datos de entrenamiento históricos, que abarcan desde 2010-02-05 hasta 2012-11-01. Dentro de este archivo encontrará los siguientes campos:


*   Tienda - el número de la tienda
*  Departamento - el número de departamento
*  Fecha - la semana
*  Weekly_Sales - ventas para el departamento dado en la tienda dada
*  IsHoliday: si la semana es una semana festiva especial

**-test.csv**

Este archivo es idéntico a train.csv, excepto que hemos retenido las ventas semanales. Debe predecir las ventas para cada triplete de tienda, departamento y fecha en este archivo.

**-caracteristicas.csv**

Este archivo contiene datos adicionales relacionados con la tienda, el departamento y la actividad regional para las fechas indicadas. Contiene los siguientes campos:

* Tienda - el número de la tienda
* Fecha - la semana
* Temperatura - temperatura media en la región
* Fuel_Price - costo del combustible en la región
* MarkDown1-5: datos anónimos relacionados con las rebajas promocionales que ejecuta Walmart. Los datos de MarkDown solo están disponibles después de noviembre de 2011 y no están disponibles para todas las tiendas todo el tiempo. Cualquier valor faltante se marca con NA.
* IPC - el índice de precios al consumidor
* Desempleo - la tasa de desempleo
* IsHoliday: si la semana es una semana festiva especial
Por conveniencia, los cuatro días festivos caen dentro de las siguientes semanas en el conjunto de datos.

Ademas no todos los días festivos están en los datos:

* Super Bowl: 12 de febrero de 2010, 11 de febrero de 2011, 10 de febrero de 2012, 8 de febrero de 2013.
* Día del Trabajo: 10-sep-10, 9-sep-11, 7-sep-12, 6-sep-13.
* Acción de gracias: 26-nov-10, 25-nov-11, 23-nov-12, 29-nov-13.
* Navidad: 31-dic-10, 30-dic-11, 28-dic-12, 27-dic-13.

# Carga de Librerias y Datos

*Se importan los módulos necesarios para trabajar*
"""

#Pandas es utilizado para leer los set de datos
import pandas as pd
#Numpy es utilizado para generar las series de datos a graficar
import numpy as np
#Matplotlib es utilizado para generar los gráficos
import matplotlib.pyplot as plt
import missingno as msno
#Seaborn es utilizado para generar los gráficos
import seaborn as sns
#Se importan modulos estadisticos para generar test de hipotesis, entre otros
# ==============================================================================
from scipy.stats import normaltest
from scipy.stats import norm
from scipy import stats
from scipy.stats import shapiro
from scipy.stats import anderson
from scipy.stats import spearmanr

# Para acceder a los archivos del gdrive
from google.colab import drive
drive.mount('/content/gdrive/')

cd /content/gdrive/MyDrive/Tesis/Datos

stores_df=pd.read_csv('stores.csv')
train_df=pd.read_csv('train.csv')
test_df=pd.read_csv('test.csv')
features_df=pd.read_csv('features.csv')

"""# Resumen de datos cargados

"""

train_df

test_df

stores_df.info()

train_df.info()

features_df.info()

"""## Verificacion de datos nulos"""

stores_df.isnull().sum()

train_df.isnull().sum()

features_df.isnull().sum()

"""**insight**: El conjunto de datos: store, train y test no presenta datos nulos, sin embargo el conjunto de datos features en las columnas de MarkDown 1-5 si presenta valores nulos.

---

# Carga data final
"""

#Union de tablas
data = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how = 'inner')
df = data.merge(stores_df, on=['Store'], how='inner')
#Se convierte la temperaruta a celsius
df['Temperature'] = df['Temperature'].apply(lambda x :  (x - 32) / 1.8)
df['Temperature'] = df['Temperature'].apply(lambda x :  (x - 32) / 1.8)
#Reemplzasa los valores Nan por 0
df.fillna(0,inplace=True)
df["Date"] = pd.to_datetime(df["Date"], dayfirst=True)

df["Year"] = df["Date"].dt.year
df["Month"] = df["Date"].dt.month
df["Day"] = df["Date"].dt.day
df["Week"] = df["Date"].dt.week

print(f'El dataset contiene {df.shape[0]} filas y {df.shape[1]} columnas')

df.info()

"""*Data final con la que se trabajara*"""

df.head(5)

"""**insight**:
* La fecha empieza el 2010/02/05 y termina el 2012/10/26  con saltos de tiempo de 1 semana a otra, siendo cada dia viernes de cada mes.
*  Cada tienda que son 45 tiene de 99 a 98 departementos.

---

#Analisis Exploratorio de Datos

##Analisis Univariado

###Se comprueba la cantidad de de registros por semana
"""

plt.subplots(figsize=(15,3))
sns.countplot(data=df, x="Week")
plt.title("Cantidad de registros por semana")
plt.show()

"""**insight**: Cada semana de la 5 hasta la 43 tiene registros estables, sin embargo hay menos registros de la semana 1 a la 4 y 44 hasta la 52, esto se explica ya que 2010 empieza en la semana 5 y 2012 termina en la semana 43.

---

*-Se comprueba la cantidad de de registros por semana y año*
"""

#Se filtra por año
df_2010=df[df['Year']==2010 ]
df_2011=df[df['Year']==2011 ]
df_2012=df[df['Year']==2012 ]

fig, (ax1,ax2,ax3) = plt.subplots(3,1,figsize=(12,7))
sns.countplot(data=df_2010, x="Week",ax=ax1)
ax1.set_title("2010")
sns.countplot(data=df_2011, x="Week",ax=ax2)
ax2.set_title("2011")
sns.countplot(data=df_2012, x="Week",ax=ax3)
ax3.set_title("2012")
plt.tight_layout()
plt.show()

"""**insight**: Cada año tiene una cantidad de registros estable.

---

###Se comprueba que no hayan datos nulos o vacios

####Se analiza la cantidad de departamentos faltantes por tienda
"""

numeracion=list(range(1,100))
faltantes_dept=[]
store=df.Store.unique()
dept_data=[]
count_dept=[]
for i in store:
  count=0
  df_filtro=df[df.Store==i]
  dept=df_filtro.Dept.unique()
  dept.sort()
  lis_dept_faltantes=[]
  count_dept.append([i,len(dept)])
  for j in numeracion:
    if j not in dept:
      lis_dept_faltantes.append(j)
      faltantes_dept.append(j)
      count=count+1
  print(f"Store: {i}, Total:{count} -Dept faltantes: {lis_dept_faltantes}")

  dept_data.append([i,count])

"""*Se analiza la cantidad de departamentos faltantes*"""

faltantes_dept_df=pd.DataFrame(faltantes_dept,columns=['dept'])
faltantes_dept_df['dept'].unique().sort()
faltantes_dept_count=[]
for i in faltantes_dept_df['dept'].unique():
  faltantes_dept_count.append([i,faltantes_dept.count(i)])

faltantes_dept_count_df=pd.DataFrame(faltantes_dept_count,columns=['dept','count'])
f,ax1 =plt.subplots(figsize=(15,4))
x=sns.barplot(x=faltantes_dept_count_df["dept"],y=faltantes_dept_count_df["count"])
plt.xticks( fontsize=8)
plt.title("Recuento del total de Departamento faltantes", fontsize=15)
plt.show()

"""*Se analiza la cantidad de departamentos faltantes por tienda*"""

dept_data_df=pd.DataFrame(dept_data,columns=["Store","Dept"])
f,ax1 =plt.subplots(figsize=(15,4))
sns.barplot(x=dept_data_df["Store"],y=dept_data_df["Dept"])
plt.xticks( fontsize=8)
plt.title("Cantidad de departmentos faltantes por tienda", fontsize=15)
plt.show()

"""*Conteo de departamentos en total por cada tienda*"""

for i in count_dept:
  print(f"Store: {i[0]}, cantidad de departamentos en total: {i[1]}")

count_dept_df=pd.DataFrame(count_dept,columns=["Store","Dept"])
f,ax1 =plt.subplots(figsize=(15,4))
sns.barplot(x=count_dept_df["Store"],y=count_dept_df["Dept"])
plt.xticks( fontsize=8)
plt.title("Cantidad de departmentos por tienda", fontsize=15)

plt.show()

"""**insight**: Se observa el detalle de los departamentos que faltan por tienda

*   El departamento N° 15, 53, 57, 61, 62, 63, 64, 66, 68, 69, 70, 73, 75, 76, 84, 86, 88, 89 no existe para ninguna tienda.
*   No todos los Store tiene todos los departamentos


---

###Se comprueba que cada Store y Departamento tengan la cantidad de registros totales de tiempo

*-Se comprueba que no falten registros de fecha, cabe recordar que la fecha empieza el 2010/02/05 y termina el 2012/10/26  con saltos de tiempo de 1 semana a otra, siendo cada dia viernes de cada mes*
"""

from datetime import date, timedelta
#Comienzo y fin de el periodo
start_dt = date(2010, 2, 5)
end_dt = date(2012,10,26	)

#Salto de tiempo, que sera de una semana
delta = timedelta(days=7)

dates = []

while start_dt <= end_dt:
    #Se agrega las fechas a dates
    dates.append(start_dt.isoformat())
    start_dt += delta
#Se obtiene el largo de la lista generada
len(dates)

"""**insight**: Hay 143 semanas en el periodo de tiempo del inicio de la base de datos hasta el final



---

*Se filtra por datos faltantes*
"""

df

series_time=[]
lista_Store=df.Store.unique()
lista_Store.sort()
lista_dept=df.Dept.unique()
lista_dept.sort()

for i in lista_Store:
  for j in lista_dept:
    #lista=[]
    test=df[(df.Store==i) & (df.Dept==j)]
    if(test.empty!=True):
      if(len(test)!=143):
        print(f"La store: {i}, y dept: {j} tiene fechas faltantes, con {len(test)} semanas en total")
        #Se filtra data
        df.drop(df[(df.Store==i) & (df.Dept==j)].index,inplace=True)

"""---
**insight**: Se comprueba que en cada Store y Depatarmento faltan semanas por contabilizar, en algunos casos son extremos de 1 a 10 registros de fecha.

###Analisis de las variable de Markdown 1 al 5
"""

fig, (ax1,ax2,ax3,ax4,ax5) = plt.subplots(5,1,figsize=(12,7))
sns.barplot(x=df["Week"],y=df["MarkDown1"],ax=ax1)
sns.barplot(x=df["Week"],y=df["MarkDown2"],ax=ax2)
sns.barplot(x=df["Week"],y=df["MarkDown3"],ax=ax3)
sns.barplot(x=df["Week"],y=df["MarkDown4"],ax=ax4)
sns.barplot(x=df["Week"],y=df["MarkDown5"],ax=ax5)
plt.tight_layout()
plt.show()

"""###Análisis de Normalidad

*Se analiza mediante graficos de dispersion el comportamiento de las variables*
"""

def grafico_distribucion(variable):
  plt.subplots(figsize=(4,3))
  sns.histplot(x=df[variable],kde=True,color="blue",bins=30)
  plt.tight_layout()
  plt.show()

grafico_distribucion("Weekly_Sales")
grafico_distribucion("Size")
grafico_distribucion("Temperature")
grafico_distribucion("CPI")
grafico_distribucion("Unemployment")

df_type=df.copy()
df_type.loc[df_type["Type"] == "A", "Type"] = 1
df_type.loc[df_type["Type"] == "B", "Type"] = 2
df_type.loc[df_type["Type"] == "C", "Type"] = 3
df_type['Type'] = df_type['Type'].astype(str).astype(int)

"""*Mediante el test de shapiro se comprueba normalidad de cada variable del dataframe*

**Shapiro-Wilk test**

Comprueba si una muestra de datos tiene una distribución gaussiana.

Supuestos:

Las observaciones en cada muestra son independientes e idénticamente distribuidas (iid).

Interpretación:

* H0: la muestra tiene una distribución gaussiana.
* H1: la muestra no tiene distribución gaussiana.
"""

def shapiro(data):
  stat, p = stats.shapiro(df_type[data])
  print("Analisis de: "+data)
  print('stat=%.3f, p=%.3f' % (stat, p))
  if p > 0.05:
    print('Probably Gaussian')
    print("")
  else:
    print('Probably not Gaussian')
    print("")

shapiro('Size')
shapiro('Temperature')
shapiro('CPI')
shapiro('Fuel_Price')
shapiro('MarkDown1')
shapiro('MarkDown2')
shapiro('MarkDown3')
shapiro('MarkDown4')
shapiro('MarkDown5')
shapiro('Weekly_Sales')
shapiro('Type')
shapiro('Dept')
shapiro('Store')

"""**insight**:Mediante el test de shapiro se comprueba que ninguna variable sigue una distribucion normal.

---

##Analasis Bivariado

### Analisis de Ventas en el tiempo
"""

#Filtro por año, agrupando por Week, sumando las ventas
weekly_sales_2010 = df[df['Year']==2010].groupby('Week')['Weekly_Sales'].sum()
weekly_sales_2011 = df[df['Year']==2011].groupby('Week')['Weekly_Sales'].sum()
weekly_sales_2012 = df[df['Year']==2012].groupby('Week')['Weekly_Sales'].sum()

plt.figure(figsize=(18,4))
plt.plot(weekly_sales_2010.index, weekly_sales_2010.values)
plt.plot(weekly_sales_2011.index, weekly_sales_2011.values)
plt.plot(weekly_sales_2012.index, weekly_sales_2012.values, '*-g')

plt.xticks(np.arange(1, 53, step=1), fontsize=10)
plt.yticks( fontsize=10)
plt.xlabel('Week of the year', fontsize=10, labelpad=20)
plt.ylabel('Sales', fontsize=20, labelpad=20)

plt.title("Sales for year", fontsize=15)
plt.legend(['2010', '2011', '2012'], fontsize=12)

f=df.groupby('Date')['Weekly_Sales'].sum().reset_index()
d=df.groupby('Date')['Weekly_Sales'].mean().reset_index()
c=df.groupby('Date')['Weekly_Sales'].var().reset_index()

fig, (ax1,ax2,ax3) = plt.subplots(3,1,figsize=(12,7))
sns.lineplot(x=f["Date"],y=f["Weekly_Sales"],ax=ax1)
sns.lineplot(x=d["Date"],y=d["Weekly_Sales"],ax=ax2)
sns.lineplot(x=c["Date"],y=c["Weekly_Sales"],ax=ax3)
plt.tight_layout()
plt.show()

"""**insight**: Se observa una tendencia del alza a fines de cada año, coindicendia con las fiestas festivas. Durante el resto del año se mantiene una cantidad de ventas estables.

---

###Análisis de ventas por Type, Size y Store

*Grafico de ventas por Store*
"""

plt.subplots(figsize=(15,4))
sns.barplot(x=df["Store"],y=df["Weekly_Sales"])
plt.show()

"""**insight**: Cada tienda tiene ventas de forma irregular.

---

*Ventas por tamaño de tienda*
"""

sizes= df.groupby(["Size","Store"])['Weekly_Sales'].sum().round(0).reset_index()
f,ax1 =plt.subplots(figsize=(15,4))
sns.barplot(x=sizes["Store"],y=sizes["Size"])
plt.xticks( fontsize=8)
plt.show()

"""**insight**: El tamaño de la tienda indica una tendencia de mas ventas.

---

*Analisis de ventas en el tiempo*
"""

def grafico_lineas_store(x,ejex,ejey):
  aux=str(x[0])
  aux_1=str(x[1])
  aux_2=str(x[2])
  for i in x :
      data_1=data[data['Store']==i]
      ax[ejex,ejey].plot(data_1['Date'], data_1['Weekly_Sales'])
      ax[ejex,ejey].set_ylabel('Mean sales')
      ax[ejex,ejey].set_xlabel('Date')
      ax[ejex,ejey].legend([aux,aux_1,aux_2])
      ax[ejex,ejey].set_title('Mean sales record by store '+aux+"--"+aux_2)

grouped=df.groupby(['Store','Date']).mean().round(0).reset_index()
grouped.shape
grouped.head()

data=grouped[['Store','Date','Weekly_Sales']]
store=df["Store"].unique()

store_1=store[0:3]
store_2=store[3:6]
store_3=store[6:9]
store_4=store[9:12]
store_5=store[12:15]
store_6=store[15:18]
store_7=store[18:21]
store_8=store[21:24]
store_9=store[24:27]
store_10=store[27:30]
store_11=store[30:33]
store_12=store[33:36]
store_13=store[36:39]
store_14=store[39:42]
store_15=store[42:45]

fig, ax = plt.subplots(5,3,figsize=(25,18))
fig.subplots_adjust(wspace=0.5, hspace=0.5)
fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)

grafico_lineas_store(store_1,0,0)
grafico_lineas_store(store_2,0,1)
grafico_lineas_store(store_3,0,2)
grafico_lineas_store(store_4,1,0)
grafico_lineas_store(store_5,1,1)
grafico_lineas_store(store_6,1,2)
grafico_lineas_store(store_7,2,0)
grafico_lineas_store(store_8,2,1)
grafico_lineas_store(store_9,2,2)
grafico_lineas_store(store_11,3,0)
grafico_lineas_store(store_12,3,1)
grafico_lineas_store(store_13,3,2)
grafico_lineas_store(store_14,4,0)
grafico_lineas_store(store_15,4,1)

plt.tight_layout()
plt.show()

"""**insight**: Se mantiene la tendencia de alza a fin de año.

---

###Análisis de ventas por tipo de Tienda
"""

def grafico_type(type):

  weekly_sales_2010_type = df[(df['Year']==2010) & (df['Type']==type)].groupby('Week')['Weekly_Sales'].mean()
  weekly_sales_2011_type = df[(df['Year']==2011) & (df['Type']==type)].groupby('Week')['Weekly_Sales'].mean()
  weekly_sales_2012_type = df[(df['Year']==2012) & (df['Type']==type)].groupby('Week')['Weekly_Sales'].mean()

  plt.figure(figsize=(20,3))
  plt.plot(weekly_sales_2010_type.index, weekly_sales_2010_type.values)
  plt.plot(weekly_sales_2011_type.index, weekly_sales_2011_type.values)
  plt.plot(weekly_sales_2012_type.index, weekly_sales_2012_type.values, '*-g')

  plt.xticks(np.arange(1, 53, step=1), fontsize=10)
  plt.yticks( fontsize=10)
  plt.xlabel('Week of the year', fontsize=10, labelpad=8)
  plt.ylabel('Sales', fontsize=15, labelpad=10)

  plt.title("Sales by type "+type, fontsize=15)
  plt.legend(['2010', '2011', '2012'], fontsize=10)

  plt.tight_layout()
  plt.show()

grafico_type("A")
grafico_type("B")
grafico_type("C")

weekly_sales_type = df.groupby('Type')['Weekly_Sales'].sum()
f, (ax1) = plt.subplots( figsize=(5, 4))
ax1.bar(weekly_sales_type.index,weekly_sales_type.values)
ax1.set_title('Sales for Type', fontsize=14)

"""**insight**: Mas ventas por tipo de tienda A

---

*Ventas por tipo de tienda*
"""

weekly_sales_type = df.groupby(["Store","Type"])['Weekly_Sales'].sum().reset_index()
plt.subplots(figsize=(20,6))
sns.barplot(data=weekly_sales_type, x="Store", y="Weekly_Sales",hue="Type")
plt.title("Sales by Store", fontsize=15)
plt.tight_layout()
plt.show()

"""**insight**: Las tiendas de tipo A tienden a tener  mas ventas respecto a las demas.

Considerando que la cantidad de tiendas:

*   Tipo A son 22
*   Tipo B son 17
*   Tipo C son 6



---
"""

weekly_sales_type = df.groupby(['Type','Year'])['Weekly_Sales'].sum().reset_index()
plt.subplots(figsize=(7,4))
sns.barplot(data=weekly_sales_type, x="Type", y="Weekly_Sales",hue="Year")
plt.title("Sales by type", fontsize=15)
plt.tight_layout()
plt.show()

"""**insight**: El año 2012 tiene menos ventas ya que este año no se registra los ultimos meses donde hay mayor peak de ventas.

---

###Análisis de ventas por departamento
"""

plt.subplots(figsize=(20,10))
sns.boxplot(x='Dept', y='Weekly_Sales', data=df, showfliers=False)
plt.title("Sales by Dept", fontsize=15)
plt.tight_layout()
plt.show()

"""**insight**: Cada departamento en la suma de ventas, tiene diferentes indicadores.

---
"""

weekly_sales_dept = df.groupby(["Dept","Type"])['Weekly_Sales'].sum().round(0).reset_index()
plt.subplots(figsize=(20,6))
sns.barplot(data=weekly_sales_dept, x="Dept", y="Weekly_Sales",hue="Type")
plt.title("Sales by Dept", fontsize=15)
plt.tight_layout()
plt.show()

"""###Análisis de otras características"""

def graphic_line_scatter(eje_x):
  f, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 3))

  sns.lineplot(x="Date", y=eje_x,data=df,ax=ax1)
  ax1.set_title("Graphic Line "+eje_x, fontsize=14)

  sns.scatterplot(x=eje_x, y="Weekly_Sales",data=df,ax=ax2)
  plt.tight_layout()
  plt.show()

graphic_line_scatter("Fuel_Price")
graphic_line_scatter("CPI")
graphic_line_scatter("Unemployment")

"""**insight**: No es concluyente la relacion de estas variables con las ventas.

---
"""

f,ax1 =plt.subplots(figsize=(10,3))
sns.scatterplot(x=df["Temperature"],y=df["Weekly_Sales"])
plt.show()

"""### Analisis Size"""

sizes= df.groupby("Size")['Weekly_Sales'].sum().round(0).reset_index()
f,ax1 =plt.subplots(figsize=(25,3))
sns.barplot(x=sizes["Size"],y=sizes["Weekly_Sales"])
plt.xticks( fontsize=8)
plt.show()

sizes= df.groupby(["Size","Type"])['Weekly_Sales'].sum().round(0).reset_index()
f,ax1 =plt.subplots(figsize=(24,3))
sns.barplot(x=sizes["Size"],y=sizes["Weekly_Sales"],hue=sizes["Type"])
plt.xticks( fontsize=8)
plt.show()

"""**insight**: Se recalca que Tipo A de tiendas tiene mayor tamaño.

---

##Analisis multivariado

###Análisis de Correlacion

*Se aplica correlacion spearman*
"""

plt.figure(figsize=(10,10))
sns.heatmap(
    df.corr("spearman"),
    annot     = True,
    cbar      = True,
    annot_kws = {"size": 8},
    vmin      = -1,
    vmax      = 1,
    center    = 0,
    cmap      = sns.diverging_palette(20, 220, n=200),
    square    = True,
)

fig, ax = plt.subplots(figsize=(3,6))
corr = df.corr('spearman')[['Weekly_Sales']].sort_values(by='Weekly_Sales', ascending=False)
sns.heatmap(corr, annot=True)

"""**insight**:

De la correlacion de Spearman se obtiene que:

*   El tamaño y el tipo estan altamente correlacionadas
*   El año con los Markdown 1-5, CPI y Fuel_price estan relacionados, ver graficos anteriores para entender

De la correlacion de Spearman con la variable objetivo de ventas semanales:

1.   Solo el tamaño de la tienda destaca.




---
"""

def spearman(data):
  stat, p = spearmanr(df['Weekly_Sales'], df[data])
  print("Analisis de: "+data)
  print('stat=%.3f, p=%.3f' % (stat, p))
  if p > 0.05:
    print('Probably independent')
    print("")
  else:
    print('Probably dependent')
    print("")

spearman('Dept')
spearman('Type')
spearman('Size')
spearman('Temperature')
spearman('CPI')
spearman('Fuel_Price')
spearman('MarkDown1')
spearman('MarkDown2')
spearman('MarkDown3')
spearman('MarkDown4')
spearman('MarkDown5')
spearman('Store')
spearman('Unemployment')
spearman('IsHoliday')

"""#Análisis de valores atípicos

##Aplicando boxplots
"""

def boxplot_data(dataframe):
  plt.figure(figsize = (20,8))
  dataframe.boxplot()
  plt.show()

boxplot_data(df)

"""**insight**: Solo "Weekly_sales" presenta valoes a atipicos


---

#Resumen de Conclusiones
"""

features = [feature for feature in df.columns if feature not in ('Day','Month','Year','Week')]
df=df[features]
df.to_csv('df.csv', index = False)

"""A partir del EDA realizado se obtiene que:

* La fecha empieza el **2010/02/05 y termina el 2012/10/26**  con saltos de tiempo de 1 semana a otra, siendo cada dia viernes de cada mes, con **143** semanas en total
* El departamento no 15, 53, 57, 61, 62, 63, 64, 66, 68, 69, 70, 73, 75, 76, 84, 86, 88, 89 no existe para ninguna tienda.
* Se comprueba no todas las semanas total estan presenten en cada Store y departamentos donde debe haber por serie 143 registros.
* Los datos no sigen una distribucion normal
* Hay peak de ventas en fechas de fin de año, durant el resto del año se mantiene  estables las ventas, respecto a cada año.
* Medriante analisis descriptivo se tiene que:El tamaño de la tienda indica una tendencia de mas ventas, donde hay mas ventas por tipo de tienda A al estar mas presente en la base de datos.
* Mediante analisis de correlacion no se encontro ninguna relacion.
* Solo hay valores atipicos en la varibable de Ventas semanales.


"""
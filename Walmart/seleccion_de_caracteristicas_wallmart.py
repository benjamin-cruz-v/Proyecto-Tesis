# -*- coding: utf-8 -*-
"""Seleccion de caracteristicas-Wallmart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OOqxVBQPe86uUXY7NxpcPDBHPhnUnO6x

**Preprocesamiento de la base de datos de Consumo Electrico**

Este archivo consta de los codigos y conclusiones de:
* 1.Separar en datos de entrenamiento y de prueba.
* 2.Preparar datos para realizar aprendizaje supervizado.

# Carga de Librerias y Datos

*Se importan los módulos necesarios para trabajar*
"""

#Pandas es utilizado para leer los set de datos
import pandas as pd
#Numpy es utilizado para generar las series de datos a graficar
import numpy as np
#Seaborn es utilizado para generar los gráficos
import seaborn as sns
import matplotlib.pyplot as plt
#Se importan modulos estadisticos para generar test de hipotesis, entre otros
from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler
#Módulos implementa funciones que evalúan el error de predicción para propósitos específicos
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_absolute_percentage_error as mape
from sklearn.metrics import mean_squared_error as mse
#Ignorar warnings
import warnings
warnings.filterwarnings("ignore")

#Dividir arreglos o matrices en subconjuntos aleatorios de tren y prueba
from sklearn.model_selection import train_test_split
from sklearn.model_selection import TimeSeriesSplit
from numpy import array

#Modelos de machine learning
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

pip install prince --quiet

import prince

# Para acceder a los archivos del gdrive
from google.colab import drive
drive.mount('/content/gdrive/')

cd /content/gdrive/MyDrive/Tesis/Datos

df=pd.read_csv('df.csv')

df.set_index('Date', inplace=True)
features = [feature for feature in df.columns if feature not in ('Day','Month','Year','Week')]
df=df[features]

df.shape

df.head(4)

df.info()

"""#2.Normalizar base de datos

El **método de puntuación z** (a menudo llamado estandarización ) transforma los datos en una distribución con una media de 0 y una desviación estándar de 1 . Cada valor estandarizado se calcula restando la media de la característica correspondiente y luego dividiendo por la desviación estándar.
"""

#Seleccion de caracteristicas
features =df[['Weekly_Sales','IsHoliday','Temperature','Fuel_Price','MarkDown1',	'MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment','Size']]

#Se define escalado
std_scaler = StandardScaler()
min_scaler= MinMaxScaler()

#Transformacion

for i in features:
  df[i] = std_scaler.fit_transform(df[i].values.reshape(-1,1))

df_pca=df.copy()
df.head()



"""#Separacion de datos"""

#Se separa conjunto en entrenamiento y prueba; sin aleatoriedad
#Dejando un %30 de la data para test
features = [feature for feature in df.columns if feature not in ('Weekly_Sales')]

X_train, X_test, Y_train, Y_test = train_test_split(df[features], df[['Weekly_Sales']], test_size=0.3, shuffle=False)

shape=len(X_train.columns)

print("Separacion de datos terminada!")

"""#Seleccion de caracteristicas"""

# Hacer una muestra aleatoria del 20% del dataset
sample = df.sample(frac=0.2, random_state=42)

sample

round(sample.describe(),2)

"""##Usando Shap con RandomForestRegressor"""

pip install shap --quiet

import shap

# Prepares a default instance of the random forest regressor
model = RandomForestRegressor()
# Fits the model on the data
model.fit(X_train, Y_train)

# Fits the explainer
explainer = shap.Explainer(model.predict, X_test)
# Calculates the SHAP values - It takes some time
shap_values = explainer(X_test)

shap.plots.bar(shap_values)

"""##Ramdom forest regresor, para RFECV"""

from sklearn.feature_selection import RFECV

estimator = RandomForestRegressor(n_estimators=150)

# Crear una instancia de RFECV con validación cruzada
rfecv = RFECV(estimator=estimator,step=3, cv=TimeSeriesSplit(n_splits=50))

# Aplicar la eliminación recursiva de características con validación cruzada
X_selected = rfecv.fit(X_train, Y_train)

# Obtener las características seleccionadas
selected_features = X_train.columns[rfecv.support_]
print(selected_features)

# Obtener las puntuaciones de importancia de características
feature_importances = rfecv.estimator_.feature_importances_

# Crear un gráfico de barras para visualizar las puntuaciones de importancia de características
plt.figure(figsize=(6, 5))
plt.barh(range(len(feature_importances)), feature_importances, tick_label=selected_features)
plt.xticks(rotation=90)
plt.xlabel('Características')
plt.ylabel('Puntuación de importancia')
plt.title('Importancia de características seleccionadas')
plt.show()

"""##Analisis de correlacion"""

!pip install phik --quiet

import phik
from phik.report import plot_correlation_matrix
from phik import report

phik_overview = df.phik_matrix()
plot_correlation_matrix(phik_overview.values,
                        x_labels=phik_overview.columns,
                        y_labels=phik_overview.index,
                        vmin=0, vmax=1, color_map="Greens",
                        title=r"correlation $\phi_K$",
                        fontsize_factor=1.5,
                        figsize=(10, 10))
plt.tight_layout()

plt.figure(figsize=(9,9))
sns.heatmap(
    df.corr("spearman"),
    annot     = True,
    cbar      = True,
    annot_kws = {"size": 8},
    vmin      = -1,
    vmax      = 1,
    center    = 0,
    cmap      = sns.diverging_palette(20, 220, n=200),
    square    = True,
)

"""#FAMD"""

sample.head(5)

famd = prince.FAMD(
    n_components=5,
    n_iter=3,
    copy=True,
    check_input=True,
    random_state=42,
    engine="sklearn",
    handle_unknown="error"  # same parameter as sklearn.preprocessing.OneHotEncoder
)
famd = famd.fit(sample)

famd.column_coordinates_

famd.column_contributions_.style.format('{:.0%}')

summary_famd = famd.eigenvalues_summary
# Quita el signo de porcentaje y convierte a flotante
summary_famd['% of variance'] = summary_famd['% of variance'].str.rstrip('%').astype('float') / 100.0
summary_famd['% of variance (cumulative)'] = summary_famd['% of variance (cumulative)'].str.rstrip('%').astype('float') / 100.0
summary_famd

import numpy as np
# Porcentaje de varianza explicada por cada componente
# ==============================================================================
print('----------------------------------------------------')
print('Porcentaje de varianza explicada por cada componente')
print('----------------------------------------------------')
print(summary_famd['% of variance'])

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 4))
ax.bar(
    x      = np.arange(len(summary_famd)) + 1,
    height = summary_famd['% of variance']
)

for x, y in zip(np.arange(len(summary_famd)) + 1, summary_famd['% of variance']):
    label = round(y, 2)
    ax.annotate(
        label,
        (x,y),
        textcoords="offset points",
        xytext=(0,10),
        ha='center'
    )

ax.set_xticks(np.arange(len(summary_famd)) + 1)
ax.set_ylim(0, 1.1)
ax.set_title('Porcentaje de varianza explicada por cada componente')
ax.set_xlabel('Componente principal')
ax.set_ylabel('Por. varianza explicada')
plt.show()

coordenadas_componentes = famd.column_coordinates_
coordenadas_componentes

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import prince
import numpy as np
from sklearn.mixture import GaussianMixture
import plotly.graph_objects as go
import plotly.graph_objects as go

# Transpone el DataFrame para que los nombres de las variables sean las columnas
coordenadas_componentes_transposed = coordenadas_componentes.T

# Gráfico 2D con Plotly
trace = go.Scatter(x=coordenadas_componentes_transposed.loc[0],
                   y=coordenadas_componentes_transposed.loc[1],
                   mode='markers+text',
                   text=coordenadas_componentes_transposed.columns,
                   textposition='top center')
layout = go.Layout(title='Gráfico 2D',
                   xaxis=dict(title='Component 1'),
                   yaxis=dict(title='Component 2'))
fig2d = go.Figure(data=[trace], layout=layout)
fig2d.show()

# Gráfico 3D con Plotly
trace = go.Scatter3d(x=coordenadas_componentes_transposed.loc[0],
                     y=coordenadas_componentes_transposed.loc[1],
                     z=coordenadas_componentes_transposed.loc[2],
                     mode='markers+text',
                     text=coordenadas_componentes_transposed.columns,
                     textposition='top center')
layout = go.Layout(title='Gráfico 3D',
                   scene=dict(xaxis=dict(title='Component 1'),
                              yaxis=dict(title='Component 2'),
                              zaxis=dict(title='Component 3')))
fig3d = go.Figure(data=[trace], layout=layout)
fig3d.show()
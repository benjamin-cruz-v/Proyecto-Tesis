# -*- coding: utf-8 -*-
"""ELM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UzV3Y32A3w4pMldy3HY-JxlcVDl_m_FR

**Modelo de red neuronal ELM**

Este archivo consta de los codigos y conclusiones de:
* 1.Carga de librerias y datos
* 2.Normalizar Datos.
* 3.Preparar datos para realizar aprendizaje supervizado.
* 4.Modelo ELM
* 5.Evaluacion del modelo

#1.Carga de Librerias y Datos

*Se importan los módulos necesarios para trabajar*
"""

#Pandas es utilizado para leer los set de datos
import pandas as pd
#Numpy es utilizado para generar las series de datos a graficar
import numpy as np
#Seaborn es utilizado para generar los gráficos
import seaborn as sns
import matplotlib.pyplot as plt
#Se importan modulos estadisticos para generar test de hipotesis, entre otros
from sklearn.preprocessing import StandardScaler
#Módulos implementa funciones que evalúan el error de predicción para propósitos específicos
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_absolute_percentage_error as mape
from sklearn.metrics import mean_squared_error as mse
#Ignorar warnings
import warnings
warnings.filterwarnings("ignore")

#Dividir arreglos o matrices en subconjuntos aleatorios de tren y prueba
from sklearn.model_selection import train_test_split

#Biblioteca de Redes Neuronales
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dropout, LSTM, Dense, Activation,Input
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.wrappers.scikit_learn import KerasRegressor

from hyperopt import Trials, STATUS_OK, tpe, hp, fmin, space_eval
from sklearn.model_selection import cross_val_score, KFold, cross_val_predict, TimeSeriesSplit
import time

# Para acceder a los archivos del gdrive
from google.colab import drive
drive.mount('/content/gdrive/')

cd /content/gdrive/MyDrive/Tesis/Datos

"""Se obtiene conjunto de datos"""

df=pd.read_csv('df.csv')
df=df.drop(['Year', 'Week', 'Day','Month','Size','Type'], axis=1)

df.set_index('Date', inplace=True)

df.info()

df.shape

#Setear semilla
np.random.seed(42)

"""#2. Obtener series de Tiempo

*Se obtiene lista de dataframe ordenados por Store y Dept*
"""

series_time=[]
lista_Store=df.Store.unique()
lista_Store.sort()
lista_dept=df.Dept.unique()
lista_dept.sort()

for i in lista_Store:
  for j in lista_dept:
    #lista=[]
    test=df[(df.Store==i) & (df.Dept==j)]
    if(test.empty!=True):
        series_time.append(test)

len(series_time)

"""#3.Normalizar base de datos

El **método de puntuación z** (a menudo llamado estandarización ) transforma los datos en una distribución con una media de 0 y una desviación estándar de 1 . Cada valor estandarizado se calcula restando la media de la característica correspondiente y luego dividiendo por la desviación estándar.
"""

#Seleccion de caracteristicas
features =features = [feature for feature in df.columns if feature not in ('Store','Dept')]

#Se define escalado
std_scaler = StandardScaler()

series_time_scaled=[]

#Transformacion
for serie in series_time:
  for i in features:
    serie[i]=std_scaler.fit_transform(serie[i].values.reshape(-1,1))
  series_time_scaled.append(serie)

for i in features:
  df[i] = std_scaler.fit_transform(df[i].values.reshape(-1,1))

series_time_scaled[0]

len(series_time_scaled)

import random
series_time_scaled=random.sample(series_time_scaled,20)

"""#4.Preparar datos para realizar aprendizaje supervizado.

La idea es modelar cada valor en función de los valores recientes anteriores, dado un retardo de tiempo dado. **Los valores futuros de una variable en una serie de tiempo dependen de sus propios rezagos y de los rezagos de otras variables.**
"""

def time_delay_embedding(series: pd.Series, n_lags: int, horizon: int):
    """
    Incrustación de retardo de tiempo
    :param series: serie de tiempo como objeto de pandas
    :param n_lags: número de valores pasados para usar como variables explicativas
    :param horizon: horizonte de pronostico
    :return:pd.DataFrame con series temporales reconstruidas
    """
    assert isinstance(series, pd.Series)

    if series.name is None:
        name = 'Series'
    else:
        name = series.name

    n_lags_iter = list(range(n_lags, -horizon, -1))

    serie_time_delay = [series.shift(i) for i in n_lags_iter]
    serie_time_delay = pd.concat(serie_time_delay, axis=1).dropna()
    serie_time_delay.columns = [f'{name}(t-{j - 1})'
                 if j > 0 else f'{name}(t+{np.abs(j) + 1})'
                 for j in n_lags_iter]

    return serie_time_delay

series_predic=[]
series_target=[]
for serie in series_time_scaled:
  serie_split = []
  for columna in serie:
    col_df = time_delay_embedding(
        serie[columna],     #Serie de tiempo
        n_lags=1,           #Numero de retrasos
        horizon=1           # Horizonte de prediccion
          )
    serie_split.append(col_df)

  serie_df = pd.concat(serie_split, axis=1).dropna()
  predictor_variables = serie_df.columns.str.contains('\(t\-')
  target_variables = serie_df.columns.str.contains('Weekly_Sales\(t\+')

  predictor_variables = serie_df.iloc[:, predictor_variables]
  target_variables = serie_df.iloc[:, target_variables]
  series_predic.append(predictor_variables)
  series_target.append(target_variables)

#Ejemplo de variables de prediccion de una serie
series_predic[0].head()

#Ejemplo de variables objetivo de una serie
series_target[0].head()

#Se separa conjunto en entrenamiento y prueba; sin aleatoriedad
#Dejando un %20 de la data para test
X_train=pd.DataFrame()
X_test=pd.DataFrame()
Y_train=pd.DataFrame()
Y_test=pd.DataFrame()

for serie,target in zip(series_predic,series_target):
  X_train_i, X_test_i, Y_train_i, Y_test_i = train_test_split(serie, target, test_size=0.2, shuffle=False)
  X_train=pd.concat([X_train, X_train_i])
  X_test=pd.concat([X_test, X_test_i])
  Y_train=pd.concat([Y_train, Y_train_i])
  Y_test=pd.concat([Y_test, Y_test_i])


shape=len(X_train.columns)

print("Separacion de datos terminada!")

"""#ELM"""

X_train_a=X_train.to_numpy()
Y_train_a=Y_train.to_numpy()
X_test_a=X_test.to_numpy()
Y_test_a=Y_test.to_numpy()

print(X_train_a.shape)
print(Y_train_a.shape)
print(X_test_a.shape)
print(Y_test_a.shape)

pip install hpelm --quiet

from hpelm import ELM
from sklearn.model_selection import cross_val_score, KFold
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

# Definir la función objetivo
def objective(params):
    n_neurons = int(params['neurons'])
    activation = params['activation']



    #kf = KFold(n_splits=5)
    tscv = TimeSeriesSplit(n_splits=5)
    scores_mse = []
    scores_rmse = []
    scores_mae= []
    scores_mape= []
    times=[]
    models=[]
    aux=1000
    #Validacion cruzada
    for train_index, test_index in tscv.split(X_train_a):
      X_train_, X_test_ = X_train_a[train_index], X_train_a[test_index]
      y_train_, y_test_ = Y_train_a[train_index], Y_train_a[test_index]

      # Crear un objeto ELM
      model =ELM(X_train_a.shape[1], 1)

      # Añadir una capa oculta con los hiperparámetros
      model.add_neurons(n_neurons, activation)

      #Entrenamiento
      start = time.time()
      model.train(X_train_, y_train_)
      end = time.time()

      #Evaluacion del modelo
      y_pred = model.predict(X_test_a)
      score_mse = mse(Y_test_a, y_pred)
      rmse = np.sqrt(score_mse)  # Calcular el RMSE
      score_mae = mae(Y_test_a, y_pred)
      score_mape= mape(Y_test_a, y_pred)


      scores_mse.append(score_mse)
      scores_mae.append(score_mae)
      scores_mape.append(score_mape)
      scores_rmse.append(rmse)

      models.append(model)

      if(score_mse<aux):
        aux=score_mse
        best_model=model
        y_pred_=y_pred

      #Tiempo de la validadion cruzada
      time_val= end- start
      times.append(time_val)

    return {'loss': np.mean(score_mse),
            'status': STATUS_OK,
            'model': best_model,
            'params': params,
            'time':times,
            'predic':y_pred_,
            'scores_mse': scores_mse,
            'scores_mae': scores_mae,
            'scores_mape': scores_mape,
            'scores_rmse': scores_rmse,
            'models':models
            }

# Definir el espacio de búsqueda de hiperparámetros
space = {
    'neurons': hp.quniform('neurons', 32, 512, 32),
    'activation': hp.choice('activation', ['sigm', 'tanh']),
}

# Optimización bayesiana
trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)

print("Los mejores hiperparámetros son: ", best)

# Los resultados completos (incluyendo los scores de validación cruzada para cada evaluación) están en el objeto 'trials'

# Obtener una lista de los resultados de todas las evaluaciones
all_results = [trial['result'] for trial in trials]

"""#Exportar resultados"""

#Del objeto all_results donde estan los resultados de cada trial de la optimizacion bayesiana se obtiene los parametros para exportalos a un csv
results=[]
trial=0 #Nº de intento de optimizacion bayesiana

for result in all_results:
  k=0 #validacion cruzada
  trial+=1
  for time,score_mse,score_mae,score_mape,score_rmse in zip(result['time'],result['scores_mse'],result['scores_rmse'],result['scores_mae'],result['scores_mape']):
    k+=1
    nameModel = "ELM" + "_"+str(result['params']['activation'])+"_"+str(result['params']['neurons'])
    results.append([nameModel,trial,k,time,score_mse,score_rmse,score_mae,score_mape])

#Se crea dataframe
results_csv=pd.DataFrame(results,columns=['nameModel','trial_optimizacion_bayesiana','Step_validacion','time','MSE','RMSE','MAE','MAPE'])

results_o = results_csv.sort_values(by='MSE', ascending=True)
results_o.head(5)

results_csv["time"].mean()

# Exportar el DataFrame como CSV
results_csv.to_csv('results_ELM_wallmart.csv')

best_model=all_results[21]["models"][3]

predic = all_results[21]['predic']

"""###Grafico de prediccion"""

def divide_array(array, num_parts):
  i=0
  j=0
  largo=int(len(array)/num_parts)
  j=largo
  series_results=[]

  x=[]
  for k in range(num_parts):
    aux= array[i:j]
    j=largo+j
    i=i+largo
    series_results.append(aux)
    x.append(aux)

  return x

series_result_predict=divide_array(predic,20)
series_result_real=divide_array(Y_test,20)

series_result_predict=series_result_predict[0:10]
series_result_real=series_result_real[0:10]
for predict, real in zip(series_result_predict,series_result_real):
  #Grafico de prediccion con el valor real
  tiempo=[x for x in range(predict.shape[0])]
  plt.figure(figsize=(8,3))
  plt.plot(tiempo,real)
  plt.ylabel('Week Sales', size=10)
  plt.plot(tiempo,predict)
  plt.xlabel('Time step', size=10)
  plt.legend(['Prediccion','Real'])
  plt.show()

"""#NARMAX

"""

predictor_variables=pd.read_csv('predictor_variables.csv', index_col='Date')
target_variables=pd.read_csv('target_variables.csv', index_col='Date')
df=pd.read_csv('df_modelo.csv', index_col='Date')

predictor_variables=predictor_variables.to_numpy()
target_variables=target_variables.to_numpy()

def divide_array(array, num_parts,largo):
  i=0
  j=0
  j=largo
  series_results=[]

  x=[]
  for k in range(20):
    aux= array[i:j]
    j=largo+j
    i=i+largo
    series_results.append(aux)
    x.append(aux)

  return x

series=divide_array(df,20,143)
predict_series=divide_array(predictor_variables,20,142)
target_series=divide_array(target_variables,20,142)

#Se da formato de entradas como: Un tensor 3D con la forma [batch, timesteps, feature]
predictor_variable=[]
target_variable=[]
for serie, target in zip(predict_series,target_series):
  predictor=np.array(serie)
  predictor_variable.append(predictor)
  target=np.array(target)
  target_variable.append(target)

  target_shape=target.shape

def narmax_data(data,modelo,predictor,shape_target,retrasos,target):
  #Se realiza prediccion
  results = modelo.predict(predictor)


  error=[]
  aux=0
  for prediccion, real in zip(results,target):
    aux=real-prediccion
    error.append(aux[0])

  df_narmax = data.iloc[1:]
  # Agregamos el arreglo como nueva columna en el DataFrame
  df_narmax['error'] = error

  return df_narmax

series_time=[]
for serie,predictor,target in zip(series,predict_series,target_series):
  aux=narmax_data(serie,best_model,predictor,target_shape,1,target)
  series_time.append(aux)

series_time[0]



"""#4.Preparar datos para realizar aprendizaje supervizado.

La idea es modelar cada valor en función de los valores recientes anteriores, dado un retardo de tiempo dado. **Los valores futuros de una variable en una serie de tiempo dependen de sus propios rezagos y de los rezagos de otras variables.**
"""

def time_delay_embedding(series: pd.Series, n_lags: int, horizon: int):
    """
    Incrustación de retardo de tiempo
    :param series: serie de tiempo como objeto de pandas
    :param n_lags: número de valores pasados para usar como variables explicativas
    :param horizon: horizonte de pronostico
    :return:pd.DataFrame con series temporales reconstruidas
    """
    assert isinstance(series, pd.Series)

    if series.name is None:
        name = 'Series'
    else:
        name = series.name

    n_lags_iter = list(range(n_lags, -horizon, -1))

    serie_time_delay = [series.shift(i) for i in n_lags_iter]
    serie_time_delay = pd.concat(serie_time_delay, axis=1).dropna()
    serie_time_delay.columns = [f'{name}(t-{j - 1})'
                 if j > 0 else f'{name}(t+{np.abs(j) + 1})'
                 for j in n_lags_iter]

    return serie_time_delay

series_predic=[]
series_target=[]
for serie in series_time_scaled:
  serie_split = []
  for columna in serie:
    col_df = time_delay_embedding(
        serie[columna],     #Serie de tiempo
        n_lags=1,           #Numero de retrasos
        horizon=1           # Horizonte de prediccion
          )
    serie_split.append(col_df)

  serie_df = pd.concat(serie_split, axis=1).dropna()
  predictor_variables = serie_df.columns.str.contains('\(t\-')
  target_variables = serie_df.columns.str.contains('Weekly_Sales\(t\+')

  predictor_variables = serie_df.iloc[:, predictor_variables]
  target_variables = serie_df.iloc[:, target_variables]
  series_predic.append(predictor_variables)
  series_target.append(target_variables)

#Ejemplo de variables de prediccion de una serie
series_predic[0].head()

#Ejemplo de variables objetivo de una serie
series_target[0].head()

#Se separa conjunto en entrenamiento y prueba; sin aleatoriedad
#Dejando un %20 de la data para test
X_train=pd.DataFrame()
X_test=pd.DataFrame()
Y_train=pd.DataFrame()
Y_test=pd.DataFrame()

for serie,target in zip(series_predic,series_target):
  X_train_i, X_test_i, Y_train_i, Y_test_i = train_test_split(serie, target, test_size=0.2, shuffle=False)
  X_train=pd.concat([X_train, X_train_i])
  X_test=pd.concat([X_test, X_test_i])
  Y_train=pd.concat([Y_train, Y_train_i])
  Y_test=pd.concat([Y_test, Y_test_i])


shape=len(X_train.columns)

print("Separacion de datos terminada!")

"""#ELM"""

X_train_a=X_train.to_numpy()
Y_train_a=Y_train.to_numpy()
X_test_a=X_test.to_numpy()
Y_test_a=Y_test.to_numpy()

print(X_train_a.shape)
print(Y_train_a.shape)
print(X_test_a.shape)
print(Y_test_a.shape)

pip install hpelm --quiet

from hpelm import ELM
from sklearn.model_selection import cross_val_score, KFold
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

# Definir la función objetivo
def objective(params):
    n_neurons = int(params['neurons'])
    activation = params['activation']



    #kf = KFold(n_splits=5)
    tscv = TimeSeriesSplit(n_splits=5)
    scores_mse = []
    scores_rmse = []
    scores_mae= []
    scores_mape= []
    times=[]
    models=[]
    #Validacion cruzada
    for train_index, test_index in tscv.split(X_train_a):
      X_train_, X_test_ = X_train_a[train_index], X_train_a[test_index]
      y_train_, y_test_ = Y_train_a[train_index], Y_train_a[test_index]

      # Crear un objeto ELM
      model =ELM(X_train_a.shape[1], 1)

      # Añadir una capa oculta con los hiperparámetros
      model.add_neurons(n_neurons, activation)

      #Entrenamiento
      start = time.time()
      model.train(X_train_, y_train_)
      end = time.time()

      #Evaluacion del modelo
      y_pred = model.predict(X_test_a)
      score_mse = mse(Y_test_a, y_pred)
      rmse = np.sqrt(score_mse)  # Calcular el RMSE
      score_mae = mae(Y_test_a, y_pred)
      score_mape= mape(Y_test_a, y_pred)


      scores_mse.append(score_mse)
      scores_mae.append(score_mae)
      scores_mape.append(score_mape)
      scores_rmse.append(rmse)

      models.append(model)

      #Tiempo de la validadion cruzada
      time_val= end- start
      times.append(time_val)

    return {'loss': np.mean(score_mse),
            'status': STATUS_OK,
            'model': model,
            'params': params,
            'time':times,
            'predic':y_pred,
            'scores_mse': scores_mse,
            'scores_mae': scores_mae,
            'scores_mape': scores_mape,
            'scores_rmse': scores_rmse,
            'models':models
            }

# Definir el espacio de búsqueda de hiperparámetros
space = {
    'neurons': hp.quniform('neurons', 32, 512, 32),
    'activation': hp.choice('activation', ['sigm', 'tanh']),
}

# Optimización bayesiana
trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)

print("Los mejores hiperparámetros son: ", best)

# Los resultados completos (incluyendo los scores de validación cruzada para cada evaluación) están en el objeto 'trials'

# Obtener una lista de los resultados de todas las evaluaciones
all_results = [trial['result'] for trial in trials]

"""#Exportar resultados"""

#Del objeto all_results donde estan los resultados de cada trial de la optimizacion bayesiana se obtiene los parametros para exportalos a un csv
results=[]
trial=0 #Nº de intento de optimizacion bayesiana

for result in all_results:
  k=0 #validacion cruzada
  trial+=1
  for time,score_mse,score_mae,score_mape,score_rmse in zip(result['time'],result['scores_mse'],result['scores_rmse'],result['scores_mae'],result['scores_mape']):
    k+=1
    nameModel = "ELM" + "_"+str(result['params']['activation'])+"_"+str(result['params']['neurons'])
    results.append([nameModel,trial,k,time,score_mse,score_rmse,score_mae,score_mape])

#Se crea dataframe
results_csv=pd.DataFrame(results,columns=['nameModel','trial_optimizacion_bayesiana','Step_validacion','time','MSE','RMSE','MAE','MAPE'])

results_o = results_csv.sort_values(by='MSE', ascending=True)
results_o.head(5)

# Exportar el DataFrame como CSV
results_csv.to_csv('Narmax_results_ELM_wallmart.csv')

best_model=all_results[23]["models"][4]

predic = all_results[23]['predic']

"""###Grafico de prediccion"""

def divide_array(array, num_parts):
  i=0
  j=0
  largo=int(len(array)/num_parts)
  j=largo
  series_results=[]

  x=[]
  for k in range(num_parts):
    aux= array[i:j]
    j=largo+j
    i=i+largo
    series_results.append(aux)
    x.append(aux)

  return x

series_result_predict=divide_array(predic,20)
series_result_real=divide_array(Y_test,20)

series_result_predict=series_result_predict[0:10]
series_result_real=series_result_real[0:10]
for predict, real in zip(series_result_predict,series_result_real):
  #Grafico de prediccion con el valor real
  tiempo=[x for x in range(predict.shape[0])]
  plt.figure(figsize=(8,3))
  plt.plot(tiempo,real)
  plt.ylabel('Week Sales', size=10)
  plt.plot(tiempo,predict)
  plt.xlabel('Time step', size=10)
  plt.legend(['Prediccion','Real'])
  plt.show()
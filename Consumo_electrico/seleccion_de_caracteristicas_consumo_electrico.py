# -*- coding: utf-8 -*-
"""Seleccion de caracteristicas-Consumo Electrico.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F2DGfla8IMOmyANqzSJubT1sHvK7_UuL

**Preprocesamiento de la base de datos de Consumo Electrico**

Este archivo consta de los codigos y conclusiones de:
* 1.Separar en datos de entrenamiento y de prueba.
* 2.Preparar datos para realizar aprendizaje supervizado.

# Carga de Librerias y Datos

*Se importan los módulos necesarios para trabajar*
"""

#Pandas es utilizado para leer los set de datos
import pandas as pd
#Numpy es utilizado para generar las series de datos a graficar
import numpy as np
#Seaborn es utilizado para generar los gráficos
import seaborn as sns
import matplotlib.pyplot as plt
#Se importan modulos estadisticos para generar test de hipotesis, entre otros
from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler
#Módulos implementa funciones que evalúan el error de predicción para propósitos específicos
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_absolute_percentage_error as mape
from sklearn.metrics import mean_squared_error as mse
#Ignorar warnings
import warnings
warnings.filterwarnings("ignore")

#Dividir arreglos o matrices en subconjuntos aleatorios de tren y prueba
from sklearn.model_selection import train_test_split
from sklearn.model_selection import TimeSeriesSplit
from numpy import array

#Modelos de machine learning
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

pip install prince --quiet

import prince

# Para acceder a los archivos del gdrive
from google.colab import drive
drive.mount('/content/gdrive/')

cd /content/gdrive/MyDrive/Tesis/Datos-2

df=pd.read_csv('df_EDA.csv',
                parse_dates={'dt':['Date','time']},
                infer_datetime_format=True,
                low_memory=False,
                index_col='dt')

df.info()

df.shape

## resampling of data over hour
df = df.resample('D').mean()
df.shape

df

"""#2.Normalizar base de datos

El **método de puntuación z** (a menudo llamado estandarización ) transforma los datos en una distribución con una media de 0 y una desviación estándar de 1 . Cada valor estandarizado se calcula restando la media de la característica correspondiente y luego dividiendo por la desviación estándar.
"""

#Seleccion de caracteristicas
features =df.columns

#Se define escalado
std_scaler = StandardScaler()
min_scaler= MinMaxScaler()

#Transformacion

for i in features:
  df[i] = min_scaler.fit_transform(df[i].values.reshape(-1,1))

df_pca=df.copy()
df.head()

df.describe()

"""#Separacion de datos"""

#Se separa conjunto en entrenamiento y prueba; sin aleatoriedad
#Dejando un %30 de la data para test
X_train, X_test, Y_train, Y_test = train_test_split(df[['Global_reactive_power', 'Voltage',
       'Global_intensity', 'Sub_metering_1', 'Sub_metering_2',
       'Sub_metering_3', 'other_consumption']], df[['Global_active_power']], test_size=0.3, shuffle=False)

shape=len(X_train.columns)

print("Separacion de datos terminada!")

"""#Seleccion de caracteristicas

##Usando Shap con RandomForestRegressor
"""

pip install shap --quiet

import shap

# Prepares a default instance of the random forest regressor
model = RandomForestRegressor(n_estimators=150)
# Fits the model on the data
model.fit(X_train, Y_train)

# Fits the explainer
explainer = shap.Explainer(model.predict, X_test)
# Calculates the SHAP values - It takes some time
shap_values = explainer(X_test)

shap.plots.bar(shap_values)

"""##Ramdom forest regresor, para RFECV"""

from sklearn.feature_selection import RFECV

estimator = RandomForestRegressor(n_estimators=150)

# Crear una instancia de RFECV con validación cruzada
rfecv = RFECV(estimator=estimator,step=3, cv=TimeSeriesSplit(n_splits=50))

# Aplicar la eliminación recursiva de características con validación cruzada
X_selected = rfecv.fit(X_train, Y_train)

# Obtener las características seleccionadas
selected_features = X_train.columns[rfecv.support_]
print(selected_features)

# Obtener las puntuaciones de importancia de características
feature_importances = rfecv.estimator_.feature_importances_

# Crear un gráfico de barras para visualizar las puntuaciones de importancia de características
plt.figure(figsize=(6, 5))
plt.barh(range(len(feature_importances)), feature_importances, tick_label=selected_features)
plt.xticks(rotation=90)
plt.xlabel('Características')
plt.ylabel('Puntuación de importancia')
plt.title('Importancia de características seleccionadas')
plt.show()

"""##Analisis de correlacion"""

plt.figure(figsize=(9,9))
sns.heatmap(
    df.corr("spearman"),
    annot     = True,
    cbar      = True,
    annot_kws = {"size": 8},
    vmin      = -1,
    vmax      = 1,
    center    = 0,
    cmap      = sns.diverging_palette(20, 220, n=200),
    square    = True,
)

fig, ax = plt.subplots(figsize=(3,6))
corr = df.corr('spearman')[['Global_active_power']].sort_values(by='Global_active_power', ascending=False)
sns.heatmap(corr, annot=True)

"""## Sección nueva"""

scores

# 1. Selección Univariante
selector = SelectKBest(score_func=f_regression, k=1)
fit = selector.fit(X_train, Y_train)
scores = pd.DataFrame(fit.scores_)
columns = pd.DataFrame(X_train.columns)
feature_scores = pd.concat([columns, scores], axis=1)
feature_scores.columns = ['Feature', 'Score']
print("Selección Univariante:\n", feature_scores.nlargest(7, 'Score'), "\n")

from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k=7)

selector.fit(X_train, Y_train)

feature_scores = selector.scores_
selected_features = selector.get_support(indices=True)

X_selected = selector.transform(X_train)
X_selected

"""#PCA"""

# Asignar las categorías
df_pca['Global_active_power'] = pd.cut(df_pca['Global_active_power'], bins=[0, 0.3, 0.7, 1], labels=['bajo', 'promedio', 'alto'], include_lowest=True)

df_pca

from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D

# Ajustar el modelo PCA
pca = PCA()
principalComponents = pca.fit_transform(X_train)

# Graficar la varianza explicada
explained_variance = pca.explained_variance_ratio_
plt.figure(figsize=(6, 4))
plt.bar(range(len(explained_variance)), explained_variance, alpha=0.5,
            align='center', label='varianza individual explicada')
plt.ylabel('Ratio de Varianza Explicada')
plt.xlabel('Componentes Principales')
plt.legend()
plt.tight_layout()

# Obtener las variables del DataFrame
variables = list(df_pca.drop('Global_active_power', axis=1).columns)
# Obtener los autovectores correspondientes a las primeras 4 componentes
autovectores = pca.components_[:4]

# Crear una tabla con las variables y los autovectores
tabla = pd.DataFrame(autovectores.T, columns=['Componente 1', 'Componente 2', 'Componente 3', 'Componente 4'], index=variables)

# Mostrar la tabla
tabla

# Graficar el porcentaje de varianza explicado
plt.figure(figsize=(15,6))
plt.plot(np.arange(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.title('Explained Variance by Components')
plt.xticks(np.arange(1, len(pca.explained_variance_ratio_)+1))
plt.xlim(1, 7)  # Establecer límites en el eje x
plt.grid(True)
plt.show()

import plotly.graph_objects as go

# Asignamos colores a cada punto de acuerdo a si el pasajero sobrevivió o no
colors = df.Global_active_power.map({'alto': 'red', 'promedio': 'green', 'bajo': 'blue'})

# Obtener las primeras 2 y 3 componentes del PCA
componentes_2d = pca.transform(X_train)[:, :2]
componentes_3d = pca.transform(X_train)[:, :3]

# Gráfico 2D
fig2d = go.Figure(data=go.Scatter(x=componentes_2d[:,0], y=componentes_2d[:,1], mode='markers', marker=dict(color=colors)))
fig2d.update_layout(title='PCA - 2D', xaxis_title='Component 1', yaxis_title='Component 2')
fig2d.show()

# Gráfico 3D
fig3d = go.Figure(data=go.Scatter3d(x=componentes_3d[:,0], y=componentes_3d[:,1], z=componentes_3d[:,2], mode='markers', marker=dict(color=colors)))
fig3d.update_layout(scene = dict(xaxis_title='Component 1', yaxis_title='Component 2', zaxis_title='Component 3'), title='PCA - 3D')
fig3d.show()

pca = prince.PCA(
    n_components=3,
    n_iter=3,
    rescale_with_mean=True,
    rescale_with_std=True,
    copy=True,
    check_input=True,
    engine='sklearn',
    random_state=42
)
pca = pca.fit(df)

pca.eigenvalues_summary

pca.column_coordinates_

coordenadas_componentes = pca.column_coordinates_

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import prince
import numpy as np
from sklearn.mixture import GaussianMixture
import plotly.graph_objects as go
import plotly.graph_objects as go

# Transpone el DataFrame para que los nombres de las variables sean las columnas
coordenadas_componentes_transposed = coordenadas_componentes.T

# Gráfico 2D con Plotly
trace = go.Scatter(x=coordenadas_componentes_transposed.loc[0],
                   y=coordenadas_componentes_transposed.loc[1],
                   mode='markers+text',
                   text=coordenadas_componentes_transposed.columns,
                   textposition='top center')
layout = go.Layout(title='Gráfico 2D',
                   xaxis=dict(title='Component 1'),
                   yaxis=dict(title='Component 2'))
fig2d = go.Figure(data=[trace], layout=layout)
fig2d.show()

# Gráfico 3D con Plotly
trace = go.Scatter3d(x=coordenadas_componentes_transposed.loc[0],
                     y=coordenadas_componentes_transposed.loc[1],
                     z=coordenadas_componentes_transposed.loc[2],
                     mode='markers+text',
                     text=coordenadas_componentes_transposed.columns,
                     textposition='top center')
layout = go.Layout(title='Gráfico 3D',
                   scene=dict(xaxis=dict(title='Component 1'),
                              yaxis=dict(title='Component 2'),
                              zaxis=dict(title='Component 3')))
fig3d = go.Figure(data=[trace], layout=layout)
fig3d.show()